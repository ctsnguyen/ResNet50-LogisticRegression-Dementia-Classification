{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B44isck9ZGjM"
      },
      "source": [
        "What does this file does.\n",
        "\n",
        "\n",
        "1.  Imports--\n",
        "2.  Designates root, Directories, and CSVs.\n",
        "3.  Makes functions to make ID recalling easier.\n",
        "4.  Upload Files(.img and .hdr files, .txt files)\n",
        "5.  Sort files by .hdr, .img, and .txt into their root paths. Put them into a pandas table to make looking up easier.\n",
        "6.  **(upload previous CSV/Excel if you want it all in one place)**\n",
        "7.  Extracts relevant data from .txt files and includes them into the pandas table.\n",
        "8.  **Volume Stuff**\n",
        "9.  Converts MRI slices into png.\n",
        "10. **(If previous CSV/EXCEL sheet was uploaded, merge data tables)**\n",
        "11. Zip up .png files for individual patient information.\n",
        "\n",
        "Outputs:\n",
        "1.  Zip files of invidual people. Including PNG files of mri scans.\n",
        "\n",
        "**Process:\n",
        "Input .ing, .hdr, .txt files from the FSL_SEG from each of the patient's folders**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtM2vJAFVNT8"
      },
      "outputs": [],
      "source": [
        "!pip -q install nibabel scikit-image torch torchvision pandas scikit-learn tqdm openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKvz0GNGq77f",
        "outputId": "bbf754a3-90d2-4a8f-8291-8c121141c096"
      },
      "outputs": [],
      "source": [
        "# ---- Paths & imports ----\n",
        "import os, re, glob, io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Imaging / preprocessing\n",
        "import nibabel as nib\n",
        "from skimage.transform import resize\n",
        "from skimage.filters import threshold_otsu\n",
        "from skimage import morphology\n",
        "from scipy import ndimage as ndi\n",
        "from PIL import Image\n",
        "\n",
        "# Repro\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Root folders\n",
        "ROOT = \"/content/oasis_inputs\"\n",
        "MRI_DIR = os.path.join(ROOT, \"mri_uploads\")     # where .hdr/.img/.txt go\n",
        "LABELS_DIR = os.path.join(ROOT, \"labels\")       # where cleaned label file goes\n",
        "OUT_ROOT = \"/content/oasis_pipeline\"            # outputs\n",
        "PNG_DIR = os.path.join(OUT_ROOT, \"pngs\")\n",
        "\n",
        "# output CSVs\n",
        "MRI_INDEX_CSV   = os.path.join(ROOT, \"mri_upload_index.csv\")\n",
        "SUBJ_FEATS_CSV  = os.path.join(OUT_ROOT, \"subject_features_from_fast.csv\")\n",
        "MANIFEST_CSV    = os.path.join(OUT_ROOT, \"manifest_slices.csv\")\n",
        "MERGED_CSV      = os.path.join(OUT_ROOT, \"manifest_with_tabular_and_labels.csv\")\n",
        "CLEAN_LABELS_CSV= os.path.join(LABELS_DIR, \"oasis_labels_cleaned.csv\")  # optional (created below)\n",
        "\n",
        "# knobs\n",
        "K_SLICES  = 5     # slices per visit\n",
        "TARGET_MM = 1.0   # resample to ~1mm\n",
        "PNG_SIZE  = 224   # ResNet input size\n",
        "\n",
        "os.makedirs(MRI_DIR, exist_ok=True)\n",
        "os.makedirs(LABELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_ROOT, exist_ok=True)\n",
        "os.makedirs(PNG_DIR, exist_ok=True)\n",
        "\n",
        "print(\"ROOT     :\", ROOT)\n",
        "print(\"MRI_DIR  :\", MRI_DIR)\n",
        "print(\"LABELS   :\", LABELS_DIR)\n",
        "print(\"OUT_ROOT :\", OUT_ROOT)\n",
        "print(\"PNG_DIR  :\", PNG_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oAdrHf81Akh"
      },
      "source": [
        "### Citations\n",
        "**AI Assitance**\n",
        "\n",
        "Used OpenAI's ChatGPT to implement loading in the data through zip paths and the google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9P_u6zCq7-Q"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Helpers: ID normalization\n",
        "# ---------------------------\n",
        "def _norm(s: str) -> str:\n",
        "    if pd.isna(s): return \"\"\n",
        "    s = str(s).strip().upper()\n",
        "    s = s.replace(\" \", \"\").replace(\"-\", \"_\")\n",
        "    return s\n",
        "\n",
        "def _visit_id_from_any(s: str) -> str:\n",
        "    if not s: return \"\"\n",
        "    m = re.search(r\"(OAS1|OAS2)_[0-9]{4}_MR[0-9]+\", str(s).upper())\n",
        "    return m.group(0) if m else \"\"\n",
        "\n",
        "def _subject_root_from_any(s: str) -> str:\n",
        "    if not s: return \"\"\n",
        "    m = re.search(r\"(OAS1|OAS2)_[0-9]{4}\", str(s).upper())\n",
        "    return m.group(0) if m else \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "wJSSjQ85rBID",
        "outputId": "d481f414-bf22-468d-a9a4-73fab643e382"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 1) Upload MRI files (.hdr/.img/.txt) — multi-select\n",
        "# ---------------------------\n",
        "print(\"  Select ALL your MRI files (multi-select ok):\")\n",
        "print(\"   - .hdr  (Analyze header)\")\n",
        "print(\"   - .img  (Analyze image)\")\n",
        "print(\"   - .txt  (FAST segmentation report from FSL_SEG)\")\n",
        "uploaded = files.upload()  # multi-select allowed\n",
        "\n",
        "# Save to MRI_DIR\n",
        "for name, content in uploaded.items():\n",
        "    dst = os.path.join(MRI_DIR, name)\n",
        "    with open(dst, \"wb\") as f:\n",
        "        f.write(content)\n",
        "    print(\"Saved:\", dst)\n",
        "\n",
        "print(\"\\nFiles uploaded:\")\n",
        "!ls -lh $MRI_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "q4szENOTrBKw",
        "outputId": "eb46f806-18c2-41c4-857d-5dca8a6a3841"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 2) Build index (per visit)\n",
        "# ---------------------------\n",
        "hdrs = sorted(glob.glob(os.path.join(MRI_DIR, \"*.hdr\")))\n",
        "imgs = sorted(glob.glob(os.path.join(MRI_DIR, \"*.img\")))\n",
        "txts = sorted(glob.glob(os.path.join(MRI_DIR, \"*.txt\")))\n",
        "\n",
        "by_vis = {}  # key: visit_id (OAS1_0001_MR1)\n",
        "\n",
        "for p in hdrs:\n",
        "    vis = _visit_id_from_any(os.path.basename(p)) or _visit_id_from_any(p)\n",
        "    if not vis: continue\n",
        "    by_vis.setdefault(vis, {})[\"hdr_path\"] = p\n",
        "    by_vis[vis][\"subject_root\"] = _subject_root_from_any(vis)\n",
        "\n",
        "for p in imgs:\n",
        "    vis = _visit_id_from_any(os.path.basename(p)) or _visit_id_from_any(p)\n",
        "    if not vis: continue\n",
        "    by_vis.setdefault(vis, {})[\"img_path\"] = p\n",
        "    by_vis[vis][\"subject_root\"] = _subject_root_from_any(vis)\n",
        "\n",
        "for p in txts:\n",
        "    # FAST detection: be lenient — just require a \"Volumes:\" line later when we parse\n",
        "    vis = _visit_id_from_any(os.path.basename(p)) or _visit_id_from_any(p)\n",
        "    if not vis:\n",
        "        # still accept; we'll parse later and try to pull visit_id from the txt body\n",
        "        continue\n",
        "    by_vis.setdefault(vis, {})\n",
        "    if \"fast_txt_path\" not in by_vis[vis]:\n",
        "        by_vis[vis][\"fast_txt_path\"] = p\n",
        "    by_vis[vis][\"subject_root\"] = _subject_root_from_any(vis)\n",
        "\n",
        "rows = []\n",
        "for vis, rec in by_vis.items():\n",
        "    rows.append({\n",
        "        \"visit_id\": vis,\n",
        "        \"subject_root\": rec.get(\"subject_root\", _subject_root_from_any(vis)),\n",
        "        \"hdr_path\": rec.get(\"hdr_path\",\"\"),\n",
        "        \"img_path\": rec.get(\"img_path\",\"\"),\n",
        "        \"fast_txt_path\": rec.get(\"fast_txt_path\",\"\")\n",
        "    })\n",
        "df_mri_index = pd.DataFrame(rows).sort_values(\"visit_id\").reset_index(drop=True)\n",
        "\n",
        "# Add any txts with no visit_id in filename by parsing later; we'll still keep them separate\n",
        "# (we'll parse every uploaded txt anyway in the FAST parser step)\n",
        "\n",
        "print(f\"Indexed {len(df_mri_index)} visits from uploads.\")\n",
        "display(df_mri_index.head(10))\n",
        "\n",
        "df_mri_index.to_csv(MRI_INDEX_CSV, index=False)\n",
        "print(\"Saved MRI index to:\", MRI_INDEX_CSV)\n",
        "\n",
        "# Sanity checks\n",
        "missing_pairs = df_mri_index[(df_mri_index[\"hdr_path\"]==\"\") & (df_mri_index[\"img_path\"]==\"\")]\n",
        "if len(missing_pairs):\n",
        "    print(\"\\n Some visits are missing both .hdr and .img (no image slices will be produced):\")\n",
        "    display(missing_pairs)\n",
        "else:\n",
        "    print(\"\\n All indexed visits have at least one volume file (hdr or img).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgtgLo5h1MH7"
      },
      "source": [
        "### Citation\n",
        "**AI Assitance**\n",
        "\n",
        "Used OpenAI's ChatGPT to help implement loading in patient MRI data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "id": "K5o-nS1DrI5c",
        "outputId": "e3f313f9-b47e-4ddd-c5cc-f30e2cfe941b"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 3) (Optional) Upload Excel/CSV with Patient ID & CDR → cleaned labels (visit-level)\n",
        "# ---------------------------\n",
        "print(\"\\n (Optional) Upload your Excel/CSV with columns: 'Patient ID' and 'CDR'\")\n",
        "print(\"    - If you skip this, we will still produce tabular features + slice PNGs.\")\n",
        "try:\n",
        "    uploaded_labels = files.upload()  # user may cancel/skip\n",
        "    if uploaded_labels:\n",
        "        labels_name = list(uploaded_labels.keys())[0]\n",
        "        labels_path = os.path.join(LABELS_DIR, labels_name)\n",
        "        with open(labels_path, \"wb\") as f:\n",
        "            f.write(uploaded_labels[labels_name])\n",
        "        print(\"Saved label file to:\", labels_path)\n",
        "\n",
        "        # Load & clean\n",
        "        if labels_name.lower().endswith(\".xlsx\"):\n",
        "            df_labels_raw = pd.read_excel(labels_path)\n",
        "        else:\n",
        "            df_labels_raw = pd.read_csv(labels_path)\n",
        "\n",
        "        df_labels = df_labels_raw.copy()\n",
        "        df_labels.columns = [c.strip() for c in df_labels.columns]\n",
        "        assert \"Patient ID\" in df_labels.columns and \"CDR\" in df_labels.columns, \"Need 'Patient ID' and 'CDR' columns\"\n",
        "\n",
        "        df_labels[\"patient_id_norm\"] = df_labels[\"Patient ID\"].apply(_norm)\n",
        "        df_labels[\"visit_id\"] = df_labels[\"patient_id_norm\"].apply(_visit_id_from_any)\n",
        "        df_labels[\"cdr\"] = df_labels[\"CDR\"]\n",
        "\n",
        "        def cdr_to_label(x):\n",
        "            try:\n",
        "                v = float(x)\n",
        "            except:\n",
        "                return None\n",
        "            if v == 0.0: return \"CN\"\n",
        "            if v >= 0.5: return \"AD\"\n",
        "            return None\n",
        "\n",
        "        df_labels[\"label\"] = df_labels[\"cdr\"].apply(cdr_to_label)\n",
        "        df_labels = df_labels.dropna(subset=[\"visit_id\",\"label\"]).reset_index(drop=True)\n",
        "        df_labels[[\"visit_id\",\"cdr\",\"label\"]].to_csv(CLEAN_LABELS_CSV, index=False)\n",
        "        print(f\"Cleaned labels saved to: {CLEAN_LABELS_CSV}\")\n",
        "        display(df_labels[[\"visit_id\",\"cdr\",\"label\"]].head(10))\n",
        "    else:\n",
        "        print(\"Skipped label upload (no file chosen).\")\n",
        "except Exception as e:\n",
        "    print(\"Skipped label upload:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "tDYJsTSprI8P",
        "outputId": "89a1f863-2721-43cf-8b49-fa1d5efc4b77"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 4) FAST parser → tabular features (visit-level)\n",
        "# ---------------------------\n",
        "def parse_fast_txt_relaxed(txt_path):\n",
        "    \"\"\"Parse FSL FAST report, requiring only a 'Volumes:' line; infer visit_id.\"\"\"\n",
        "    try:\n",
        "        with open(txt_path, \"r\", errors=\"ignore\") as f:\n",
        "            text = f.read()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "    # find the Volumes: line (case-insensitive)\n",
        "    vols_line = next((ln for ln in text.splitlines() if ln.strip().lower().startswith(\"volumes:\")), None)\n",
        "    if not vols_line:\n",
        "        return None\n",
        "\n",
        "    # numbers: CSF, GM, WM in mm^3 typically\n",
        "    nums = re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", vols_line)\n",
        "    if len(nums) < 3:\n",
        "        return None\n",
        "\n",
        "    # visit_id inference from text or path\n",
        "    vi = _visit_id_from_any(text) or _visit_id_from_any(txt_path)\n",
        "    subj = _subject_root_from_any(text) or _subject_root_from_any(txt_path)\n",
        "\n",
        "    csf_mm3, gm_mm3, wm_mm3 = map(float, nums[:3])\n",
        "    mm3_to_ml = 1/1000.0\n",
        "    csf_ml, gm_ml, wm_ml = csf_mm3*mm3_to_ml, gm_mm3*mm3_to_ml, wm_mm3*mm3_to_ml\n",
        "    brain_ml = gm_ml + wm_ml\n",
        "\n",
        "    return {\n",
        "        \"visit_id\": vi,\n",
        "        \"subject_root\": subj,\n",
        "        \"csf_ml\": csf_ml, \"gm_ml\": gm_ml, \"wm_ml\": wm_ml, \"brain_ml\": brain_ml,\n",
        "        \"csf_ratio\": csf_ml/brain_ml if brain_ml>0 else np.nan,\n",
        "        \"gm_ratio\":  gm_ml/brain_ml if brain_ml>0 else np.nan,\n",
        "        \"wm_ratio\":  wm_ml/brain_ml if brain_ml>0 else np.nan,\n",
        "        \"gm_wm_ratio\": gm_ml/wm_ml if wm_ml>0 else np.nan,\n",
        "        \"source_txt\": os.path.basename(txt_path)\n",
        "    }\n",
        "\n",
        "# Parse every uploaded txt (including ones not matched earlier)\n",
        "all_txts = sorted(glob.glob(os.path.join(MRI_DIR, \"*.txt\")))\n",
        "feat_rows = []\n",
        "for t in tqdm(all_txts, desc=\"Parsing FAST txt → tabular\"):\n",
        "    d = parse_fast_txt_relaxed(t)\n",
        "    if d: feat_rows.append(d)\n",
        "\n",
        "df_feats = pd.DataFrame(feat_rows)\n",
        "# Ensure we have a row for every visit in df_mri_index (even if no FAST txt)\n",
        "if not df_mri_index.empty:\n",
        "    df_feats = df_mri_index[[\"visit_id\",\"subject_root\"]].merge(\n",
        "        df_feats, on=[\"visit_id\",\"subject_root\"], how=\"left\"\n",
        "    )\n",
        "\n",
        "# Deduplicate visits (keep the row with most non-null values)\n",
        "if not df_feats.empty:\n",
        "    df_feats[\"nonnull\"] = df_feats.notna().sum(axis=1)\n",
        "    df_feats = (df_feats.sort_values([\"visit_id\",\"nonnull\"], ascending=[True, False])\n",
        "                        .drop(columns=[\"nonnull\"])\n",
        "                        .drop_duplicates(subset=[\"visit_id\"], keep=\"first\"))\n",
        "\n",
        "df_feats.to_csv(SUBJ_FEATS_CSV, index=False)\n",
        "print(\"Tabular features saved →\", SUBJ_FEATS_CSV, df_feats.shape)\n",
        "display(df_feats.head(8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "fqvl8kIHrJGw",
        "outputId": "5495ae52-6315-47db-9a53-d6b1f38a5c47"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 5) Volume → CENTRAL slice PNGs, build manifest (best for AD)\n",
        "# ---------------------------\n",
        "import os, re, glob\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from skimage.transform import resize\n",
        "from skimage.filters import threshold_otsu\n",
        "from skimage import morphology\n",
        "from scipy import ndimage as ndi\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Respect prior knobs if they exist\n",
        "try: TARGET_MM\n",
        "except NameError: TARGET_MM = 1.0\n",
        "try: PNG_SIZE\n",
        "except NameError: PNG_SIZE = 224\n",
        "try: MANIFEST_CSV\n",
        "except NameError: MANIFEST_CSV = \"/content/oasis_pipeline/manifest_slices.csv\"\n",
        "try: PNG_DIR\n",
        "except NameError:\n",
        "    PNG_DIR = \"/content/oasis_pipeline/pngs\"\n",
        "    os.makedirs(PNG_DIR, exist_ok=True)\n",
        "\n",
        "CENTRAL_WIDTH_MM   = 60.0    # keep slices within ±30 mm of brain mid-plane\n",
        "DESIRED_CENTRAL_K  = 12      # ~12 central slices per visit\n",
        "\n",
        "# ---- helpers defined here so this cell is self-contained\n",
        "def _score_volume_name(name: str) -> int:\n",
        "    \"\"\"Rank likely T1 volumes; exclude seg/prob maps.\"\"\"\n",
        "    n = name.lower()\n",
        "    if any(t in n for t in [\"_seg\", \"fseg\", \"probseg\", \"prob\", \"pve\"]):\n",
        "        return -1\n",
        "    if \"masked_gfc\" in n:   return 100\n",
        "    if n.startswith(\"mpr\"): return 90\n",
        "    if \"t1\" in n:           return 80\n",
        "    return 10\n",
        "\n",
        "def _prefer_t1_in_folder(hdr_or_img_path: str) -> str:\n",
        "    \"\"\"Given one file path, search its folder for the best anatomical (T1) volume.\"\"\"\n",
        "    if not isinstance(hdr_or_img_path, str) or not len(hdr_or_img_path):\n",
        "        return \"\"\n",
        "    folder = os.path.dirname(hdr_or_img_path)\n",
        "    if not os.path.isdir(folder):  # if it's just a file in MRI_DIR, still try folder\n",
        "        folder = os.path.dirname(hdr_or_img_path)\n",
        "    cands = []\n",
        "    for ext in (\"*.hdr\", \"*.nii\", \"*.nii.gz\", \"*.img\"):\n",
        "        cands += glob.glob(os.path.join(folder, ext))\n",
        "    ranked = sorted([( _score_volume_name(os.path.basename(p)), p) for p in cands ], reverse=True)\n",
        "    for sc, p in ranked:\n",
        "        if sc >= 0:\n",
        "            return p\n",
        "    return hdr_or_img_path if os.path.exists(hdr_or_img_path) else (ranked[0][1] if ranked else \"\")\n",
        "\n",
        "def load_any(path):\n",
        "    img = nib.load(path)\n",
        "    vol = img.get_fdata().astype(np.float32)\n",
        "    if vol.ndim == 4:\n",
        "        vol = vol[..., 0]\n",
        "    vox = tuple(float(z) for z in img.header.get_zooms()[:3])\n",
        "    affine = img.affine if hasattr(img, \"affine\") else None\n",
        "    return vol, vox, affine\n",
        "\n",
        "def resample_iso(vol, voxel_mm, target_mm=1.0):\n",
        "    sx, sy, sz = voxel_mm\n",
        "    nx = int(round(vol.shape[0]*sx/target_mm))\n",
        "    ny = int(round(vol.shape[1]*sy/target_mm))\n",
        "    nz = int(round(vol.shape[2]*sz/target_mm))\n",
        "    nx = max(nx, 32); ny = max(ny, 32); nz = max(nz, 16)\n",
        "    return resize(vol, (nx, ny, nz), preserve_range=True, anti_aliasing=True)\n",
        "\n",
        "def norm_robust(vol, p1=1, p99=99):\n",
        "    a = vol[np.isfinite(vol)]\n",
        "    lo, hi = np.percentile(a, [p1, p99]) if a.size else (0, 1)\n",
        "    vol = np.clip(vol, lo, hi)\n",
        "    return ((vol - lo) / (hi - lo + 1e-8)).astype(np.float32)\n",
        "\n",
        "def skull_strip(vol_norm):\n",
        "    a = vol_norm[np.isfinite(vol_norm)]\n",
        "    thr = threshold_otsu(a) if a.size else 0.0\n",
        "    mask = vol_norm > thr\n",
        "    mask = morphology.binary_closing(mask, morphology.ball(2))\n",
        "    mask = ndi.binary_fill_holes(mask)\n",
        "    lbl, n = ndi.label(mask)\n",
        "    if n > 0:\n",
        "        sizes = ndi.sum(mask, lbl, index=np.arange(1, n+1))\n",
        "        keep = 1 + int(np.argmax(sizes))\n",
        "        mask = (lbl == keep)\n",
        "    return mask.astype(np.uint8)\n",
        "\n",
        "def choose_central_slices(mask, vox, desired_k=12, central_width_mm=60.0):\n",
        "    \"\"\"Return z indices only inside the central window; evenly spaced to ~desired_k.\"\"\"\n",
        "    sx, sy, sz = vox\n",
        "    Z = mask.shape[2]\n",
        "    z_has_brain = np.where(mask.sum(axis=(0,1)) > 0)[0]\n",
        "    if z_has_brain.size == 0:\n",
        "        mid = Z // 2\n",
        "        return [z for z in [mid-2, mid-1, mid, mid+1, mid+2] if 0 <= z < Z]\n",
        "    mid = int(np.median(z_has_brain))\n",
        "    half_vox = int(round((central_width_mm/2.0) / max(sz, 1e-6)))\n",
        "    left, right = max(0, mid - half_vox), min(Z - 1, mid + half_vox)\n",
        "    if right <= left:\n",
        "        return [z for z in [mid-2, mid-1, mid, mid+1, mid+2] if 0 <= z < Z]\n",
        "    k = max(5, min(desired_k, right - left + 1))\n",
        "    idxs = np.linspace(left, right, k).astype(int).tolist()\n",
        "    return sorted(set(int(i) for i in idxs if 0 <= i < Z))\n",
        "\n",
        "def slice_height_metrics(mask, vox, z_idxs, central_width_mm=60.0, affine=None):\n",
        "    sx, sy, sz = vox\n",
        "    z_has_brain = np.where(mask.sum(axis=(0,1)) > 0)[0]\n",
        "    zmin, zmax = int(z_has_brain.min()), int(z_has_brain.max())\n",
        "    brain_height_vox = (zmax - zmin + 1)\n",
        "    brain_height_mm = brain_height_vox * sz\n",
        "    z_mid = int(np.median(z_has_brain))\n",
        "    half_win_mm = central_width_mm / 2.0\n",
        "    X, Y, Z = mask.shape\n",
        "    cx, cy = (X - 1) / 2.0, (Y - 1) / 2.0\n",
        "    rows = []\n",
        "    for z in z_idxs:\n",
        "        z_mm_from_min = (z - zmin) * sz\n",
        "        z_frac = float(np.clip(z_mm_from_min / max(brain_height_mm, 1e-6), 0, 1))\n",
        "        dist_mid_mm = abs((z - z_mid) * sz)\n",
        "        is_central = dist_mid_mm <= half_win_mm\n",
        "        if affine is not None:\n",
        "            ijk = np.array([cx, cy, float(z), 1.0], dtype=np.float32)\n",
        "            xyz = affine @ ijk\n",
        "            world_z_mm = float(xyz[2])\n",
        "        else:\n",
        "            world_z_mm = np.nan\n",
        "        rows.append(dict(\n",
        "            z_index=int(z),\n",
        "            brain_height_mm=float(brain_height_mm),\n",
        "            z_mm_from_brain_min=float(z_mm_from_min),\n",
        "            z_frac_0to1=z_frac,\n",
        "            dist_to_mid_mm=float(dist_mid_mm),\n",
        "            is_central=bool(is_central),\n",
        "            world_z_mm=world_z_mm\n",
        "        ))\n",
        "    return rows\n",
        "\n",
        "def slice_to_png(sl2d, out_path, size=224):\n",
        "    s = resize(sl2d, (size, size), preserve_range=True, anti_aliasing=True)\n",
        "    s = np.clip(s, 0, 1)\n",
        "    u8 = (s * 255).astype(np.uint8)\n",
        "    rgb = np.stack([u8]*3, axis=-1)\n",
        "    Image.fromarray(rgb).save(out_path, \"PNG\")\n",
        "\n",
        "# ---- central-only export\n",
        "slice_rows = []\n",
        "\n",
        "for _, r in tqdm(df_mri_index.iterrows(), total=len(df_mri_index), desc=\"Exporting CENTRAL PNG slices\"):\n",
        "    # Prefer true T1 in the same folder; avoid *_seg/*_fseg\n",
        "    base_path = r[\"hdr_path\"] if isinstance(r[\"hdr_path\"], str) and len(r[\"hdr_path\"]) else r[\"img_path\"]\n",
        "    vol_path  = _prefer_t1_in_folder(base_path)\n",
        "\n",
        "    if not isinstance(vol_path, str) or not len(vol_path) or not os.path.exists(vol_path):\n",
        "        print(\"Missing/invalid volume for visit:\", r.get(\"visit_id\"), \"— skipping\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        vol, vox, affine = load_any(vol_path)\n",
        "        vol = resample_iso(vol, vox, target_mm=TARGET_MM)\n",
        "        vol = norm_robust(vol)\n",
        "        mask = skull_strip(vol)\n",
        "\n",
        "        # 1) choose only central candidates\n",
        "        z_idxs = choose_central_slices(mask, vox,\n",
        "                                       desired_k=DESIRED_CENTRAL_K,\n",
        "                                       central_width_mm=CENTRAL_WIDTH_MM)\n",
        "\n",
        "        # 2) compute metrics and filter to is_central == True\n",
        "        metrics_all = slice_height_metrics(mask, vox, z_idxs,\n",
        "                                           central_width_mm=CENTRAL_WIDTH_MM,\n",
        "                                           affine=affine)\n",
        "        metrics = [m for m in metrics_all if m[\"is_central\"]]\n",
        "        z_idxs  = [m[\"z_index\"] for m in metrics]\n",
        "\n",
        "        # 3) fallback: if none inside window, take 5 closest-to-mid brain slices\n",
        "        if len(z_idxs) == 0:\n",
        "            z_has_brain = np.where(mask.sum(axis=(0,1)) > 0)[0]\n",
        "            if z_has_brain.size == 0:\n",
        "                continue\n",
        "            z_mid = int(np.median(z_has_brain))\n",
        "            cands = sorted(list(set(z_has_brain)), key=lambda z: abs(z - z_mid))[:5]\n",
        "            metrics = slice_height_metrics(mask, vox, cands,\n",
        "                                           central_width_mm=CENTRAL_WIDTH_MM,\n",
        "                                           affine=affine)\n",
        "            metrics = sorted(metrics, key=lambda m: abs(m[\"dist_to_mid_mm\"]))[:5]\n",
        "            z_idxs  = [m[\"z_index\"] for m in metrics]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Volume failed for visit {r.get('visit_id')}: {vol_path} -> {e}\")\n",
        "        continue\n",
        "\n",
        "    visit_id = r[\"visit_id\"]\n",
        "    subj_root = r.get(\"subject_root\", visit_id.split(\"_MR\")[0])\n",
        "\n",
        "    for z, m in zip(z_idxs, metrics):\n",
        "        sl = vol[:, :, z]\n",
        "        if (sl > 0).sum() < 50:\n",
        "            continue\n",
        "        png_name = f\"{visit_id}_central_ax{z}.png\"\n",
        "        out_path = os.path.join(PNG_DIR, png_name)\n",
        "        slice_to_png(sl, out_path, size=PNG_SIZE)\n",
        "\n",
        "        slice_rows.append({\n",
        "            \"visit_id\": visit_id,\n",
        "            \"subject_root\": subj_root,\n",
        "            \"png_path\": out_path,\n",
        "            \"slice_index\": int(z),\n",
        "            \"brain_height_mm\": m[\"brain_height_mm\"],\n",
        "            \"z_mm_from_brain_min\": m[\"z_mm_from_brain_min\"],\n",
        "            \"z_frac_0to1\": m[\"z_frac_0to1\"],\n",
        "            \"dist_to_mid_mm\": m[\"dist_to_mid_mm\"],\n",
        "            \"is_central\": True,                     # guaranteed\n",
        "            \"world_z_mm\": m[\"world_z_mm\"]\n",
        "        })\n",
        "\n",
        "df_manifest = pd.DataFrame(slice_rows)\n",
        "df_manifest.to_csv(MANIFEST_CSV, index=False)\n",
        "print(\"CENTRAL-ONLY manifest →\", MANIFEST_CSV, df_manifest.shape)\n",
        "display(df_manifest.head(10))\n",
        "print(\"PNG dir:\", PNG_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jip64xVs11li"
      },
      "source": [
        "### Citation\n",
        "**AI Assistance**\n",
        "\n",
        "Used OpenAI ChatGPT to help create multiple slices of MRI patient scans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "OduOFkc-rBNf",
        "outputId": "c995fc1b-a9d6-4e30-928e-a8a19bc042f7"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 6) (Optional) Merge with labels (if provided)\n",
        "# ---------------------------\n",
        "if os.path.exists(CLEAN_LABELS_CSV):\n",
        "    df_labels_clean = pd.read_csv(CLEAN_LABELS_CSV)\n",
        "    labels_visit = df_labels_clean[[\"visit_id\",\"cdr\",\"label\"]].drop_duplicates()\n",
        "    df_all = (df_manifest\n",
        "              .merge(df_feats, on=[\"visit_id\",\"subject_root\"], how=\"left\")\n",
        "              .merge(labels_visit, on=\"visit_id\", how=\"left\"))\n",
        "    df_all.to_csv(MERGED_CSV, index=False)\n",
        "    print(\"Merged (images + tabular + labels) →\", MERGED_CSV, df_all.shape)\n",
        "    display(df_all.head(10))\n",
        "else:\n",
        "    # Still save a merged file without labels for later\n",
        "    df_all = df_manifest.merge(df_feats, on=[\"visit_id\",\"subject_root\"], how=\"left\")\n",
        "    df_all.to_csv(MERGED_CSV, index=False)\n",
        "    print(\"Labels not found; saved merged (images + tabular) without labels →\", MERGED_CSV, df_all.shape)\n",
        "    display(df_all.head(10))\n",
        "\n",
        "print(\"\\n Done. Outputs:\")\n",
        "print(\"   • Tabular features:\", SUBJ_FEATS_CSV)\n",
        "print(\"   • Slice PNGs dir  :\", PNG_DIR)\n",
        "print(\"   • Slice manifest  :\", MANIFEST_CSV)\n",
        "print(\"   • Merged CSV      :\", MERGED_CSV, \"(with labels if you uploaded them)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEs54O3V1UEN"
      },
      "source": [
        "### Citation\n",
        "**AI Assistance**\n",
        "\n",
        "Used OpenAI ChatGPT to help make the optional cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxRU8KF1q8Ak",
        "outputId": "a4f7b2c0-3008-4a81-c6b4-98a6eb5ec662"
      },
      "outputs": [],
      "source": [
        "import os, re, glob, shutil\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "PNG_DIR = \"/content/oasis_pipeline/pngs\"\n",
        "BY_VISIT_DIR = \"/content/oasis_pipeline/pngs_by_visit\"\n",
        "EXPORT_ZIPS_DIR = \"/content/oasis_pipeline/exports\"\n",
        "os.makedirs(BY_VISIT_DIR, exist_ok=True)\n",
        "os.makedirs(EXPORT_ZIPS_DIR, exist_ok=True)\n",
        "\n",
        "def _visit_id_from_any(s: str) -> str:\n",
        "    m = re.search(r\"(OAS1|OAS2)_[0-9]{4}_MR[0-9]+\", str(s).upper())\n",
        "    return m.group(0) if m else \"\"\n",
        "\n",
        "# 1) Build mapping: visit_id -> list of png paths\n",
        "pngs = sorted(glob.glob(os.path.join(PNG_DIR, \"*.png\")))\n",
        "visit_map = {}\n",
        "\n",
        "use_manifest = ('df_manifest' in globals()) and ('visit_id' in df_manifest.columns)\n",
        "if use_manifest:\n",
        "    print(\"Using df_manifest to map visit_ids → PNGs\")\n",
        "    for _, r in df_manifest.iterrows():\n",
        "        vi = r.get(\"visit_id\") or _visit_id_from_any(r.get(\"png_path\"))\n",
        "        if not vi:\n",
        "            continue\n",
        "        visit_map.setdefault(vi, []).append(r[\"png_path\"])\n",
        "else:\n",
        "    print(\"No df_manifest with visit_id found; parsing visit_id from filenames\")\n",
        "    for p in pngs:\n",
        "        vi = _visit_id_from_any(os.path.basename(p)) or _visit_id_from_any(p) or \"_unknown_visit\"\n",
        "        visit_map.setdefault(vi, []).append(p)\n",
        "\n",
        "_has_feats = ('df_feats' in globals()) and (not pd.DataFrame(df_feats).empty)\n",
        "_has_labels = ('labels_visit' in globals()) and (not pd.DataFrame(labels_visit).empty)\n",
        "\n",
        "# 2) Copy PNGs + write per-visit CSVs\n",
        "print(f\"Organizing {sum(len(v) for v in visit_map.values())} PNGs into folders and writing CSVs…\")\n",
        "for vi, paths in tqdm(visit_map.items()):\n",
        "    out_dir = os.path.join(BY_VISIT_DIR, vi)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # Copy PNGs\n",
        "    for src in paths:\n",
        "        if not isinstance(src, str) or not os.path.exists(src):\n",
        "            continue\n",
        "        dst = os.path.join(out_dir, os.path.basename(src))\n",
        "        if not os.path.exists(dst):\n",
        "            try:\n",
        "                shutil.copy2(src, dst)\n",
        "            except Exception as e:\n",
        "                print(\"copy failed:\", src, \"->\", e)\n",
        "\n",
        "    # Build per-visit manifest CSV\n",
        "    if use_manifest:\n",
        "        df_v = df_manifest[df_manifest['visit_id'] == vi].copy()\n",
        "        # Attach labels if present\n",
        "        if _has_labels:\n",
        "            df_v = df_v.merge(labels_visit, on='visit_id', how='left')\n",
        "        # Save\n",
        "        man_csv = os.path.join(out_dir, f\"manifest_{vi}.csv\")\n",
        "        df_v.to_csv(man_csv, index=False)\n",
        "    else:\n",
        "        # Minimal manifest from filenames\n",
        "        df_v = pd.DataFrame({\n",
        "            \"visit_id\": [vi]*len(paths),\n",
        "            \"png_path\": [os.path.join(out_dir, os.path.basename(p)) for p in paths]\n",
        "        })\n",
        "        man_csv = os.path.join(out_dir, f\"manifest_{vi}.csv\")\n",
        "        df_v.to_csv(man_csv, index=False)\n",
        "\n",
        "    # Build per-visit tabular CSV (FAST-derived volumes/ratios)\n",
        "    if _has_feats:\n",
        "        df_tab = df_feats[df_feats['visit_id'] == vi].copy()\n",
        "        if df_tab.empty:\n",
        "            # create a 1-row placeholder so downstream code doesn't break\n",
        "            df_tab = pd.DataFrame([{\"visit_id\": vi}])\n",
        "        tab_csv = os.path.join(out_dir, f\"tabular_{vi}.csv\")\n",
        "        df_tab.to_csv(tab_csv, index=False)\n",
        "\n",
        "print(\"Per-visit folders with PNGs + CSVs created at:\", BY_VISIT_DIR)\n",
        "\n",
        "# 3) Zip each visit\n",
        "print(\"Zipping each visit…\")\n",
        "made = []\n",
        "for vi in tqdm(sorted(visit_map.keys())):\n",
        "    folder = os.path.join(BY_VISIT_DIR, vi)\n",
        "    if not os.path.isdir(folder):\n",
        "        continue\n",
        "    zip_base = os.path.join(EXPORT_ZIPS_DIR, vi)\n",
        "    shutil.make_archive(zip_base, 'zip', folder)\n",
        "    made.append(zip_base + \".zip\")\n",
        "\n",
        "print(f\"Created {len(made)} ZIPs in:\", EXPORT_ZIPS_DIR)\n",
        "\n",
        "# 4) One big ZIP with all per-visit folders\n",
        "BIG_ZIP_BASE = os.path.join(EXPORT_ZIPS_DIR, \"all_visits_packages\")\n",
        "shutil.make_archive(BIG_ZIP_BASE, 'zip', BY_VISIT_DIR)\n",
        "BIG_ZIP = BIG_ZIP_BASE + \".zip\"\n",
        "print(\"Big ZIP:\", BIG_ZIP)\n",
        "\n",
        "# 5) Show sizes for convenience\n",
        "def _fmt_size(bytes_):\n",
        "    for unit in ['B','KB','MB','GB','TB']:\n",
        "        if bytes_ < 1024.0:\n",
        "            return f\"{bytes_:3.1f} {unit}\"\n",
        "        bytes_ /= 1024.0\n",
        "    return f\"{bytes_:3.1f} PB\"\n",
        "\n",
        "print(\"\\n ZIP file sizes:\")\n",
        "for z in made[:10]:\n",
        "    try:\n",
        "        print(f\" - {os.path.basename(z)}  {_fmt_size(os.path.getsize(z))}\")\n",
        "    except:\n",
        "        pass\n",
        "print(f\" - {os.path.basename(BIG_ZIP)}  {_fmt_size(os.path.getsize(BIG_ZIP))}\")\n",
        "\n",
        "# Optional download prompt for the big zip:\n",
        "# from google.colab import files\n",
        "# files.download(BIG_ZIP)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft3U1SPj19pJ"
      },
      "source": [
        "### Citation\n",
        "**AI Assistance**\n",
        "\n",
        "Used OpenAI ChatGPT to help make a zip file with all the extracted data from each patient data file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUtszC2Qkx6Y"
      },
      "source": [
        "# Citation\n",
        "OpenAI. (2025). ChatGPT (Nov 25 version) [Large language model]\n",
        "\n",
        "ChatGPT assistance for help with implementing the syntax for making the preprocessing code and uploading the patient data files process and extract the necessary features [Large language model]\n",
        "\n",
        "Conversations with the user within November 2025\n",
        "\n",
        "Link to ChatGPT: https://chatgpt.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJ-jqNMq3EY2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
